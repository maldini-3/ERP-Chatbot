from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from Retrieval import retrieve_relevant_chunks

MODEL_NAME = "Qwen/Qwen1.5-7B-Chat"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, device_map="auto", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

def generate_response(user_query):
    retrieved_chunks = retrieve_relevant_chunks(user_query)
    best_chunk = retrieved_chunks[0] if retrieved_chunks else ""
    
    if not best_chunk.strip():
        return "âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ."
    
    prompt = f"""
    Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_query}
    
    Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
    {best_chunk.strip()}
    
    Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø¹Ù„Ø§Ù‡ØŒ Ø£Ø¬Ø¨ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙˆØ¯Ù‚Ø© Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±Ø©.
    """
    
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024).to(device)
    output = model.generate(
        **inputs, max_new_tokens=150, repetition_penalty=1.2, temperature=0.7, top_p=0.9, do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    return tokenizer.decode(output[0], skip_special_tokens=True).strip()

if __name__ == "__main__":
    print("ğŸŸ¢ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ÙƒØªØ§Ø¨Ø© Ø£Ø³Ø¦Ù„ØªÙƒ (Ø§ÙƒØªØ¨ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬)")
    while True:
        user_input = input("ğŸ‘¤ You: ").strip()
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ ÙˆØ¯Ø§Ø¹Ù‹Ø§!")
            break
        elif user_input:
            response = generate_response(user_input)
            print(f"\n----------------------\nğŸ¤– ERP Chatbot: {response}\n----------------------\n")
        else:
            print("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ ØµØ§Ù„Ø­.")
